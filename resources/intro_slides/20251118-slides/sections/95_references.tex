\section{参考文献}

\begin{frame}{参考文献}
\scriptsize
\begin{itemize}
  \item Jin, Z., Chen, Y., Leeb, F., et al. (2023). CLADDER: Assessing causal reasoning in language models. \emph{NeurIPS}.
  \item Roemmele, M., Bejan, C. A., \& Gordon, A. S. (2011). Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In \emph{AAAI Spring Symp. on Logical Formalizations of Commonsense Reasoning}.
  \item Jin, Z., Liu, J., Lyu, Z., et al. (2023). Can large language models infer causation from correlation? \emph{Findings of ACL 2023}.
  \item Romanou, A., Montariol, S., Paul, D., et al. (2023). CRAB: Assessing the strength of causal relationships between real-world events. \emph{Findings of EMNLP 2023}.
  \item Frohberg, J., \& Binder, F. (2022). CRASS: A novel data set and benchmark to test counterfactual reasoning of large language models. In \emph{LREC 2022} (pp.~2126--2140).
  \item Du, L., Ding, X., Xiong, K., et al. (2022). e-CARE: A new dataset for exploring explainable causal reasoning. \emph{Findings of ACL 2022}.
  \item Pain / e-CARE（同源临床因果任务）: 参见上述 e-CARE 数据集条目（Du et al., 2022），Pain 任务为同源临床因果推理场景扩展。
  \item Nie, A., et al. (2023). MoCa: Measuring human--language model alignment on causal and moral judgment tasks. \emph{NeurIPS}, 36.
  \item Wang, Y., \& Zhao, Y. (2023). TRAM: Benchmarking temporal reasoning for large language models. \emph{arXiv}:2310.00835.
\end{itemize}
\end{frame}